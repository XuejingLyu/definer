{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch Implementation of Deep Deterministic Policy Gradient\n",
    "\n",
    "Application to dynamic liquidity minting strategy (Coming soon...)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "635ee0d2d6bab21d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:52:13.188808Z",
     "start_time": "2024-01-09T07:52:10.999822Z"
    }
   },
   "id": "959a03f405e105"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "EPISODES = 200\n",
    "EP_STEPS = 200\n",
    "LR_ACTOR = 0.001\n",
    "LR_CRITIC = 0.002\n",
    "GAMMA = 0.9\n",
    "TAU = 0.01\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "RENDER = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:52:13.195490Z",
     "start_time": "2024-01-09T07:52:13.191733Z"
    }
   },
   "id": "bb24cafc51e02664"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module): # define the network structure for actor and critic\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_dim, 30)\n",
    "        self.fc1.weight.data.normal_(0, 0.1) # initialization of FC1\n",
    "        self.out = nn.Linear(30, a_dim)\n",
    "        self.out.weight.data.normal_(0, 0.1) # initilizaiton of OUT\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = torch.tanh(x)\n",
    "        actions = x * 2 # for the game \"Pendulum-v0\", action range is [-2, 2]\n",
    "        return actions\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.fcs = nn.Linear(s_dim, 30)\n",
    "        self.fcs.weight.data.normal_(0, 0.1)\n",
    "        self.fca = nn.Linear(a_dim, 30)\n",
    "        self.fca.weight.data.normal_(0, 0.1)\n",
    "        self.out = nn.Linear(30, 1)\n",
    "        self.out.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "    def forward(self, s, a):\n",
    "        x = self.fcs(s)\n",
    "        y = self.fca(a)\n",
    "        actions_value = self.out(F.relu(x+y))\n",
    "        return actions_value\n",
    "    \n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound):\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0 # serves as updating the memory data \n",
    "        \n",
    "        # Create the 4 network objects\n",
    "        self.actor_eval = ActorNet(s_dim, a_dim)\n",
    "        self.actor_target = ActorNet(s_dim, a_dim)\n",
    "        self.critic_eval = CriticNet(s_dim, a_dim)\n",
    "        self.critic_target = CriticNet(s_dim, a_dim)\n",
    "        \n",
    "        # create 2 optimizers for actor and critic\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_eval.parameters(), lr=LR_ACTOR)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_eval.parameters(), lr=LR_CRITIC)\n",
    "        \n",
    "        # Define the loss function for critic network update\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_next): # how to store the episodic data to buffer\n",
    "        transition = np.hstack((state, action, [reward], state_next))\n",
    "        index = self.pointer % MEMORY_CAPACITY # replace the old data with new data \n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
    "        return self.actor_eval(state)[0].detach()\n",
    "    \n",
    "    def learn(self):\n",
    "        # softly update the target networks\n",
    "        for x in self.actor_target.state_dict().keys():\n",
    "            eval('self.actor_target.' + x + '.data.mul_((1-TAU))')\n",
    "            eval('self.actor_target.' + x + '.data.add_(TAU*self.actor_eval.' + x + '.data)')\n",
    "        for x in self.critic_target.state_dict().keys():\n",
    "            eval('self.critic_target.' + x + '.data.mul_((1-TAU))')\n",
    "            eval('self.critic_target.' + x + '.data.add_(TAU*self.critic_eval.' + x + '.data)')   \n",
    "            \n",
    "        # sample from buffer a mini-batch data\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        batch_trans = self.memory[indices, :]\n",
    "        \n",
    "        # extract data from mini-batch of transitions including s, a, r, s_\n",
    "        batch_s = torch.FloatTensor(batch_trans[:, :self.s_dim])\n",
    "        batch_a = torch.FloatTensor(batch_trans[:, self.s_dim:self.s_dim + self.a_dim])\n",
    "        batch_r = torch.FloatTensor(batch_trans[:, -self.s_dim - 1: -self.s_dim])\n",
    "        batch_s_ = torch.FloatTensor(batch_trans[:, -self.s_dim:])\n",
    "        \n",
    "        # make action and evaluate its action values\n",
    "        a = self.actor_eval(batch_s)\n",
    "        q = self.critic_eval(batch_s, a)\n",
    "        actor_loss = -torch.mean(q)\n",
    "        \n",
    "        # optimize the loss of actor network\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # compute the target Q value using the information of next state\n",
    "        a_target = self.actor_target(batch_s_)\n",
    "        q_tmp = self.critic_target(batch_s_, a_target)\n",
    "        q_target = batch_r + GAMMA * q_tmp\n",
    "        \n",
    "        # compute the current q value and the loss\n",
    "        q_eval = self.critic_eval(batch_s, batch_a)\n",
    "        td_error = self.loss_func(q_target, q_eval)\n",
    "        \n",
    "        # optimize the loss of critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.critic_optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:52:13.209344Z",
     "start_time": "2024-01-09T07:52:13.194022Z"
    }
   },
   "id": "fe8cf45830d8d56d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<gym.envs.classic_control.pendulum.PendulumEnv at 0x7fa52cb06ac0>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the env in gym\n",
    "ENV_NAME = 'Pendulum-v1'\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "# env.seed(1)\n",
    "env"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:52:13.224709Z",
     "start_time": "2024-01-09T07:52:13.210479Z"
    }
   },
   "id": "3177d5155e7e87ea"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "a_bound = env.action_space.high\n",
    "a_low_bound = env.action_space.low"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:52:13.231597Z",
     "start_time": "2024-01-09T07:52:13.225735Z"
    }
   },
   "id": "731fe7323dc1c01f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:53:30.342489Z",
     "start_time": "2024-01-09T07:52:13.228335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 \t Reward: -1049 \t Explore: 3.00\n",
      "Episode:  1 \t Reward: -1542 \t Explore: 3.00\n",
      "Episode:  2 \t Reward: -1521 \t Explore: 3.00\n",
      "Episode:  3 \t Reward: -1016 \t Explore: 3.00\n",
      "Episode:  4 \t Reward: -1393 \t Explore: 3.00\n",
      "Episode:  5 \t Reward: -1221 \t Explore: 3.00\n",
      "Episode:  6 \t Reward: -1361 \t Explore: 3.00\n",
      "Episode:  7 \t Reward: -1425 \t Explore: 3.00\n",
      "Episode:  8 \t Reward: -1124 \t Explore: 3.00\n",
      "Episode:  9 \t Reward: -1107 \t Explore: 3.00\n",
      "Episode:  10 \t Reward: -1177 \t Explore: 3.00\n",
      "Episode:  11 \t Reward: -1044 \t Explore: 3.00\n",
      "Episode:  12 \t Reward: -1184 \t Explore: 3.00\n",
      "Episode:  13 \t Reward: -1442 \t Explore: 3.00\n",
      "Episode:  14 \t Reward: -1434 \t Explore: 3.00\n",
      "Episode:  15 \t Reward: -1061 \t Explore: 3.00\n",
      "Episode:  16 \t Reward: -1392 \t Explore: 3.00\n",
      "Episode:  17 \t Reward: -1183 \t Explore: 3.00\n",
      "Episode:  18 \t Reward: -1100 \t Explore: 3.00\n",
      "Episode:  19 \t Reward: -828 \t Explore: 3.00\n",
      "Episode:  20 \t Reward: -1165 \t Explore: 3.00\n",
      "Episode:  21 \t Reward: -1495 \t Explore: 3.00\n",
      "Episode:  22 \t Reward: -1032 \t Explore: 3.00\n",
      "Episode:  23 \t Reward: -865 \t Explore: 3.00\n",
      "Episode:  24 \t Reward: -1580 \t Explore: 3.00\n",
      "Episode:  25 \t Reward: -1002 \t Explore: 3.00\n",
      "Episode:  26 \t Reward: -967 \t Explore: 3.00\n",
      "Episode:  27 \t Reward: -1454 \t Explore: 3.00\n",
      "Episode:  28 \t Reward: -1550 \t Explore: 3.00\n",
      "Episode:  29 \t Reward: -1461 \t Explore: 3.00\n",
      "Episode:  30 \t Reward: -1299 \t Explore: 3.00\n",
      "Episode:  31 \t Reward: -1566 \t Explore: 3.00\n",
      "Episode:  32 \t Reward: -1411 \t Explore: 3.00\n",
      "Episode:  33 \t Reward: -1530 \t Explore: 3.00\n",
      "Episode:  34 \t Reward: -1439 \t Explore: 3.00\n",
      "Episode:  35 \t Reward: -978 \t Explore: 3.00\n",
      "Episode:  36 \t Reward: -1626 \t Explore: 3.00\n",
      "Episode:  37 \t Reward: -1548 \t Explore: 3.00\n",
      "Episode:  38 \t Reward: -1036 \t Explore: 3.00\n",
      "Episode:  39 \t Reward: -1071 \t Explore: 3.00\n",
      "Episode:  40 \t Reward: -1178 \t Explore: 3.00\n",
      "Episode:  41 \t Reward: -1170 \t Explore: 3.00\n",
      "Episode:  42 \t Reward: -1005 \t Explore: 3.00\n",
      "Episode:  43 \t Reward: -1438 \t Explore: 3.00\n",
      "Episode:  44 \t Reward: -972 \t Explore: 3.00\n",
      "Episode:  45 \t Reward: -1523 \t Explore: 3.00\n",
      "Episode:  46 \t Reward: -914 \t Explore: 3.00\n",
      "Episode:  47 \t Reward: -1364 \t Explore: 3.00\n",
      "Episode:  48 \t Reward: -1192 \t Explore: 3.00\n",
      "Episode:  49 \t Reward: -1336 \t Explore: 3.00\n",
      "Episode:  50 \t Reward: -1067 \t Explore: 2.71\n",
      "Episode:  51 \t Reward: -1632 \t Explore: 2.46\n",
      "Episode:  52 \t Reward: -1503 \t Explore: 2.22\n",
      "Episode:  53 \t Reward: -1567 \t Explore: 2.01\n",
      "Episode:  54 \t Reward: -1679 \t Explore: 1.82\n",
      "Episode:  55 \t Reward: -1255 \t Explore: 1.65\n",
      "Episode:  56 \t Reward: -1385 \t Explore: 1.49\n",
      "Episode:  57 \t Reward: -1147 \t Explore: 1.35\n",
      "Episode:  58 \t Reward: -1019 \t Explore: 1.22\n",
      "Episode:  59 \t Reward: -1071 \t Explore: 1.10\n",
      "Episode:  60 \t Reward: -1324 \t Explore: 1.00\n",
      "Episode:  61 \t Reward: -1124 \t Explore: 0.90\n",
      "Episode:  62 \t Reward: -1014 \t Explore: 0.82\n",
      "Episode:  63 \t Reward: -907 \t Explore: 0.74\n",
      "Episode:  64 \t Reward: -1005 \t Explore: 0.67\n",
      "Episode:  65 \t Reward: -558 \t Explore: 0.61\n",
      "Episode:  66 \t Reward: -591 \t Explore: 0.55\n",
      "Episode:  67 \t Reward: -836 \t Explore: 0.50\n",
      "Episode:  68 \t Reward: -141 \t Explore: 0.45\n",
      "Episode:  69 \t Reward: -271 \t Explore: 0.41\n",
      "Episode:  70 \t Reward: -265 \t Explore: 0.37\n",
      "Episode:  71 \t Reward: -527 \t Explore: 0.33\n",
      "Episode:  72 \t Reward: -377 \t Explore: 0.30\n",
      "Episode:  73 \t Reward: -124 \t Explore: 0.27\n",
      "Episode:  74 \t Reward: -505 \t Explore: 0.25\n",
      "Episode:  75 \t Reward: -259 \t Explore: 0.22\n",
      "Episode:  76 \t Reward: -271 \t Explore: 0.20\n",
      "Episode:  77 \t Reward: -267 \t Explore: 0.18\n",
      "Episode:  78 \t Reward: -244 \t Explore: 0.16\n",
      "Episode:  79 \t Reward: -129 \t Explore: 0.15\n",
      "Episode:  80 \t Reward: -137 \t Explore: 0.14\n",
      "Episode:  81 \t Reward: -3 \t Explore: 0.12\n",
      "Episode:  82 \t Reward: -1 \t Explore: 0.11\n",
      "Episode:  83 \t Reward: -293 \t Explore: 0.10\n",
      "Episode:  84 \t Reward: -262 \t Explore: 0.09\n",
      "Episode:  85 \t Reward: -250 \t Explore: 0.08\n",
      "Episode:  86 \t Reward: -246 \t Explore: 0.07\n",
      "Episode:  87 \t Reward: -258 \t Explore: 0.07\n",
      "Episode:  88 \t Reward: -246 \t Explore: 0.06\n",
      "Episode:  89 \t Reward: -125 \t Explore: 0.05\n",
      "Episode:  90 \t Reward: -131 \t Explore: 0.05\n",
      "Episode:  91 \t Reward: -131 \t Explore: 0.04\n",
      "Episode:  92 \t Reward: -129 \t Explore: 0.04\n",
      "Episode:  93 \t Reward: -257 \t Explore: 0.04\n",
      "Episode:  94 \t Reward: -118 \t Explore: 0.03\n",
      "Episode:  95 \t Reward: -135 \t Explore: 0.03\n",
      "Episode:  96 \t Reward: -345 \t Explore: 0.03\n",
      "Episode:  97 \t Reward: 0 \t Explore: 0.02\n",
      "Episode:  98 \t Reward: -1242 \t Explore: 0.02\n",
      "Episode:  99 \t Reward: -3 \t Explore: 0.02\n",
      "Episode:  100 \t Reward: -131 \t Explore: 0.02\n",
      "Episode:  101 \t Reward: -656 \t Explore: 0.02\n",
      "Episode:  102 \t Reward: -5 \t Explore: 0.01\n",
      "Episode:  103 \t Reward: -130 \t Explore: 0.01\n",
      "Episode:  104 \t Reward: -300 \t Explore: 0.01\n",
      "Episode:  105 \t Reward: -2 \t Explore: 0.01\n",
      "Episode:  106 \t Reward: -132 \t Explore: 0.01\n",
      "Episode:  107 \t Reward: -132 \t Explore: 0.01\n",
      "Episode:  108 \t Reward: -136 \t Explore: 0.01\n",
      "Episode:  109 \t Reward: -2 \t Explore: 0.01\n",
      "Episode:  110 \t Reward: -128 \t Explore: 0.01\n",
      "Episode:  111 \t Reward: -2 \t Explore: 0.01\n",
      "Episode:  112 \t Reward: -246 \t Explore: 0.01\n",
      "Episode:  113 \t Reward: -252 \t Explore: 0.00\n",
      "Episode:  114 \t Reward: -270 \t Explore: 0.00\n",
      "Episode:  115 \t Reward: -140 \t Explore: 0.00\n",
      "Episode:  116 \t Reward: -3 \t Explore: 0.00\n",
      "Episode:  117 \t Reward: -138 \t Explore: 0.00\n",
      "Episode:  118 \t Reward: -388 \t Explore: 0.00\n",
      "Episode:  119 \t Reward: -266 \t Explore: 0.00\n",
      "Episode:  120 \t Reward: -3 \t Explore: 0.00\n",
      "Episode:  121 \t Reward: 0 \t Explore: 0.00\n",
      "Episode:  122 \t Reward: -247 \t Explore: 0.00\n",
      "Episode:  123 \t Reward: -276 \t Explore: 0.00\n",
      "Episode:  124 \t Reward: -389 \t Explore: 0.00\n",
      "Episode:  125 \t Reward: -136 \t Explore: 0.00\n",
      "Episode:  126 \t Reward: -143 \t Explore: 0.00\n",
      "Episode:  127 \t Reward: -10 \t Explore: 0.00\n",
      "Episode:  128 \t Reward: -141 \t Explore: 0.00\n",
      "Episode:  129 \t Reward: -142 \t Explore: 0.00\n",
      "Episode:  130 \t Reward: -141 \t Explore: 0.00\n",
      "Episode:  131 \t Reward: -693 \t Explore: 0.00\n",
      "Episode:  132 \t Reward: -11 \t Explore: 0.00\n",
      "Episode:  133 \t Reward: -11 \t Explore: 0.00\n",
      "Episode:  134 \t Reward: -811 \t Explore: 0.00\n",
      "Episode:  135 \t Reward: -689 \t Explore: 0.00\n",
      "Episode:  136 \t Reward: -279 \t Explore: 0.00\n",
      "Episode:  137 \t Reward: -830 \t Explore: 0.00\n",
      "Episode:  138 \t Reward: -144 \t Explore: 0.00\n",
      "Episode:  139 \t Reward: -947 \t Explore: 0.00\n",
      "Episode:  140 \t Reward: -138 \t Explore: 0.00\n",
      "Episode:  141 \t Reward: -1015 \t Explore: 0.00\n",
      "Episode:  142 \t Reward: -1058 \t Explore: 0.00\n",
      "Episode:  143 \t Reward: -1150 \t Explore: 0.00\n",
      "Episode:  144 \t Reward: -1387 \t Explore: 0.00\n",
      "Episode:  145 \t Reward: -10 \t Explore: 0.00\n",
      "Episode:  146 \t Reward: -12 \t Explore: 0.00\n",
      "Episode:  147 \t Reward: -958 \t Explore: 0.00\n",
      "Episode:  148 \t Reward: -549 \t Explore: 0.00\n",
      "Episode:  149 \t Reward: -566 \t Explore: 0.00\n",
      "Episode:  150 \t Reward: -280 \t Explore: 0.00\n",
      "Episode:  151 \t Reward: -277 \t Explore: 0.00\n",
      "Episode:  152 \t Reward: -139 \t Explore: 0.00\n",
      "Episode:  153 \t Reward: -675 \t Explore: 0.00\n",
      "Episode:  154 \t Reward: -139 \t Explore: 0.00\n",
      "Episode:  155 \t Reward: -10 \t Explore: 0.00\n",
      "Episode:  156 \t Reward: -394 \t Explore: 0.00\n",
      "Episode:  157 \t Reward: -144 \t Explore: 0.00\n",
      "Episode:  158 \t Reward: -144 \t Explore: 0.00\n",
      "Episode:  159 \t Reward: -271 \t Explore: 0.00\n",
      "Episode:  160 \t Reward: -275 \t Explore: 0.00\n",
      "Episode:  161 \t Reward: -236 \t Explore: 0.00\n",
      "Episode:  162 \t Reward: -1535 \t Explore: 0.00\n",
      "Episode:  163 \t Reward: -1531 \t Explore: 0.00\n",
      "Episode:  164 \t Reward: -1059 \t Explore: 0.00\n",
      "Episode:  165 \t Reward: -616 \t Explore: 0.00\n",
      "Episode:  166 \t Reward: -514 \t Explore: 0.00\n",
      "Episode:  167 \t Reward: -605 \t Explore: 0.00\n",
      "Episode:  168 \t Reward: -665 \t Explore: 0.00\n",
      "Episode:  169 \t Reward: -731 \t Explore: 0.00\n",
      "Episode:  170 \t Reward: -673 \t Explore: 0.00\n",
      "Episode:  171 \t Reward: -722 \t Explore: 0.00\n",
      "Episode:  172 \t Reward: -886 \t Explore: 0.00\n",
      "Episode:  173 \t Reward: -138 \t Explore: 0.00\n",
      "Episode:  174 \t Reward: -403 \t Explore: 0.00\n",
      "Episode:  175 \t Reward: -535 \t Explore: 0.00\n",
      "Episode:  176 \t Reward: -270 \t Explore: 0.00\n",
      "Episode:  177 \t Reward: -553 \t Explore: 0.00\n",
      "Episode:  178 \t Reward: -1527 \t Explore: 0.00\n",
      "Episode:  179 \t Reward: -140 \t Explore: 0.00\n",
      "Episode:  180 \t Reward: -279 \t Explore: 0.00\n",
      "Episode:  181 \t Reward: -402 \t Explore: 0.00\n",
      "Episode:  182 \t Reward: -143 \t Explore: 0.00\n",
      "Episode:  183 \t Reward: -399 \t Explore: 0.00\n",
      "Episode:  184 \t Reward: -453 \t Explore: 0.00\n",
      "Episode:  185 \t Reward: -138 \t Explore: 0.00\n",
      "Episode:  186 \t Reward: -275 \t Explore: 0.00\n",
      "Episode:  187 \t Reward: -273 \t Explore: 0.00\n",
      "Episode:  188 \t Reward: -411 \t Explore: 0.00\n",
      "Episode:  189 \t Reward: -1138 \t Explore: 0.00\n",
      "Episode:  190 \t Reward: -282 \t Explore: 0.00\n",
      "Episode:  191 \t Reward: -270 \t Explore: 0.00\n",
      "Episode:  192 \t Reward: -137 \t Explore: 0.00\n",
      "Episode:  193 \t Reward: -132 \t Explore: 0.00\n",
      "Episode:  194 \t Reward: -429 \t Explore: 0.00\n",
      "Episode:  195 \t Reward: -130 \t Explore: 0.00\n",
      "Episode:  196 \t Reward: -399 \t Explore: 0.00\n",
      "Episode:  197 \t Reward: -680 \t Explore: 0.00\n",
      "Episode:  198 \t Reward: -1 \t Explore: 0.00\n",
      "Episode:  199 \t Reward: -782 \t Explore: 0.00\n",
      "Running time:  77.10373711585999\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "var = 3 # the controller of exploration which will decay during training process\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    s = env.reset()[0]\n",
    "    ep_r = 0\n",
    "    for j in range(EP_STEPS):\n",
    "        # if RENDER: env.render() \n",
    "        # add explorative noise to action\n",
    "        a = ddpg.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), a_low_bound, a_bound)\n",
    "        s_, r, done, info, _ = env.step(a)\n",
    "        ddpg.store_transition(s, a, r / 10, s_) # store the transition to memory\n",
    "        \n",
    "        if ddpg.pointer > MEMORY_CAPACITY:\n",
    "            var *= 0.9995 # decay the exploration controller factor\n",
    "            ddpg.learn()\n",
    "            \n",
    "        s = s_\n",
    "        ep_r += r\n",
    "        \n",
    "    print('Episode: ', i, '\\t Reward: %i' % ep_r, '\\t Explore: %.2f' % var)\n",
    "    # if ep_r > -300 : RENDER = True\n",
    "\n",
    "print('Running time: ', time.time() - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
